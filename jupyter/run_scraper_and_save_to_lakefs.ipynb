{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21619b-7f31-4a6d-af8c-92e3fc9ec035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: selenium in /opt/conda/lib/python3.11/site-packages (4.32.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/conda/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (20.0.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/conda/lib/python3.11/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/conda/lib/python3.11/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/conda/lib/python3.11/site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.11/site-packages (from s3fs) (2.22.0)\n",
      "Requirement already satisfied: fsspec==2025.3.2.* in /opt/conda/lib/python3.11/site-packages (from s3fs) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs) (3.11.18)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.4,>=1.37.2 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/conda/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->webdriver-manager) (3.3.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas selenium webdriver-manager pyarrow s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e6faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:21:56,867 - INFO - 3296181972.py:204 - Configured LakeFS Target Path: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:21:56,867 - INFO - 3296181972.py:205 - Scraping interval set to 5 minutes.\n",
      "2025-05-18 20:21:56,868 - INFO - 3296181972.py:27 - Initializing WebDriver...\n",
      "2025-05-18 20:21:56,869 - INFO - logger.py:11 - ====== WebDriver manager ======\n",
      "2025-05-18 20:21:57,002 - INFO - logger.py:11 - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-18 20:21:57,216 - INFO - logger.py:11 - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-18 20:21:57,395 - INFO - logger.py:11 - Driver [/home/jovyan/.wdm/drivers/chromedriver/linux64/136.0.7103.94/chromedriver-linux64/chromedriver] found in cache\n",
      "2025-05-18 20:21:57,893 - INFO - 3296181972.py:40 - WebDriver initialized successfully.\n",
      "2025-05-18 20:21:57,895 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:21:57,901 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:21:58,494 - INFO - 3296181972.py:119 - Successfully read 25 rows from existing Parquet file.\n",
      "2025-05-18 20:21:58,496 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:22:00,130 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:22:10,136 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:22:10', 'display_date_id': '11940', 'display_time': '03:19', 'current_value_MW': 22063.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:22:10,137 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:22:10', 'display_date_id': '11940', 'display_time': '03:19', 'current_value_MW': 22063.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:22:10,149 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:22:10,156 - INFO - 3296181972.py:150 - Rows after deduplication: 25\n",
      "/tmp/ipykernel_1838/3296181972.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deduplicated_df['temp_datetime_sort'] = pd.to_datetime(\n",
      "/tmp/ipykernel_1838/3296181972.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deduplicated_df.sort_values(\n",
      "/tmp/ipykernel_1838/3296181972.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deduplicated_df.drop(columns=['temp_datetime_sort'], inplace=True)\n",
      "2025-05-18 20:22:10,169 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:22:10,174 - INFO - 3296181972.py:175 - Saving 25 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:22:10,210 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:22:10,211 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:22:10,214 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:27:10,205 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:27:10,209 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:27:10,277 - INFO - 3296181972.py:119 - Successfully read 25 rows from existing Parquet file.\n",
      "2025-05-18 20:27:10,278 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:27:10,416 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:27:20,421 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:27:20', 'display_date_id': '12300', 'display_time': '03:25', 'current_value_MW': 21785.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:27:20,422 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:27:20', 'display_date_id': '12300', 'display_time': '03:25', 'current_value_MW': 21785.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:27:20,427 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:27:20,429 - INFO - 3296181972.py:150 - Rows after deduplication: 26\n",
      "2025-05-18 20:27:20,433 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:27:20,434 - INFO - 3296181972.py:175 - Saving 26 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:27:20,460 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:27:20,461 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:27:20,462 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:32:20,446 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:32:20,449 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:32:20,543 - INFO - 3296181972.py:119 - Successfully read 26 rows from existing Parquet file.\n",
      "2025-05-18 20:32:20,545 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:32:20,809 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:32:30,813 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:32:30', 'display_date_id': '12660', 'display_time': '03:31', 'current_value_MW': 21932.0, 'temperature_C': 29.1}\n",
      "2025-05-18 20:32:30,814 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:32:30', 'display_date_id': '12660', 'display_time': '03:31', 'current_value_MW': 21932.0, 'temperature_C': 29.1}\n",
      "2025-05-18 20:32:30,819 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:32:30,824 - INFO - 3296181972.py:150 - Rows after deduplication: 27\n",
      "2025-05-18 20:32:30,830 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:32:30,831 - INFO - 3296181972.py:175 - Saving 27 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:32:30,864 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:32:30,865 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:32:30,866 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:37:30,851 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:37:30,853 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:37:30,911 - INFO - 3296181972.py:119 - Successfully read 27 rows from existing Parquet file.\n",
      "2025-05-18 20:37:30,912 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:37:31,007 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:37:41,011 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:37:41', 'display_date_id': '12900', 'display_time': '03:35', 'current_value_MW': 21841.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:37:41,012 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:37:41', 'display_date_id': '12900', 'display_time': '03:35', 'current_value_MW': 21841.1, 'temperature_C': 29.1}\n",
      "2025-05-18 20:37:41,017 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:37:41,020 - INFO - 3296181972.py:150 - Rows after deduplication: 28\n",
      "2025-05-18 20:37:41,025 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:37:41,026 - INFO - 3296181972.py:175 - Saving 28 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:37:41,047 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:37:41,047 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:37:41,048 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:42:41,028 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:42:41,030 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:42:41,091 - INFO - 3296181972.py:119 - Successfully read 28 rows from existing Parquet file.\n",
      "2025-05-18 20:42:41,093 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:42:41,238 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:42:51,248 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:42:51', 'display_date_id': '13260', 'display_time': '03:41', 'current_value_MW': 21676.8, 'temperature_C': 29.0}\n",
      "2025-05-18 20:42:51,248 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:42:51', 'display_date_id': '13260', 'display_time': '03:41', 'current_value_MW': 21676.8, 'temperature_C': 29.0}\n",
      "2025-05-18 20:42:51,251 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:42:51,254 - INFO - 3296181972.py:150 - Rows after deduplication: 29\n",
      "2025-05-18 20:42:51,260 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:42:51,262 - INFO - 3296181972.py:175 - Saving 29 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:42:51,285 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:42:51,286 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:42:51,286 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:47:51,266 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 20:47:51,267 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:47:51,342 - INFO - 3296181972.py:119 - Successfully read 29 rows from existing Parquet file.\n",
      "2025-05-18 20:47:51,345 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 20:47:51,775 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 20:48:01,781 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 20:48:01', 'display_date_id': '13500', 'display_time': '03:45', 'current_value_MW': 21567.4, 'temperature_C': 29.0}\n",
      "2025-05-18 20:48:01,782 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 20:48:01', 'display_date_id': '13500', 'display_time': '03:45', 'current_value_MW': 21567.4, 'temperature_C': 29.0}\n",
      "2025-05-18 20:48:01,785 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 20:48:01,787 - INFO - 3296181972.py:150 - Rows after deduplication: 30\n",
      "2025-05-18 20:48:01,791 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 20:48:01,792 - INFO - 3296181972.py:175 - Saving 30 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 20:48:01,817 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 20:48:01,818 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 20:48:01,818 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 20:51:33,075 - INFO - 3296181972.py:222 - Process interrupted by user (KeyboardInterrupt).\n",
      "2025-05-18 20:51:33,099 - INFO - 3296181972.py:100 - Closing WebDriver.\n",
      "2025-05-18 20:51:33,103 - WARNING - connectionpool.py:872 - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc502b12450>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2a81de13515e1c8cb2c0fcf315ecb651\n",
      "2025-05-18 20:51:33,111 - WARNING - connectionpool.py:872 - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc500844150>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2a81de13515e1c8cb2c0fcf315ecb651\n",
      "2025-05-18 20:51:33,121 - WARNING - connectionpool.py:872 - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc54f107350>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2a81de13515e1c8cb2c0fcf315ecb651\n",
      "2025-05-18 20:51:33,127 - INFO - 3296181972.py:228 - Scraping process terminated.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# --- EGATRealTimeScraper Class ---\n",
    "class EGATRealTimeScraper:\n",
    "    def __init__(self, url=\"https://www.sothailand.com/sysgen/egat/\"):\n",
    "        self.url = url\n",
    "        self.driver = None\n",
    "        self._initialize_driver()\n",
    "\n",
    "    def _initialize_driver(self):\n",
    "        logging.info(\"Initializing WebDriver...\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--log-level=0\")\n",
    "        chrome_options.set_capability('goog:loggingPrefs', {'browser': 'ALL'})\n",
    "\n",
    "        try:\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logging.info(\"WebDriver initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing WebDriver: {e}\", exc_info=True)\n",
    "            self.driver = None\n",
    "\n",
    "    def extract_data_from_console(self):\n",
    "        if not self.driver:\n",
    "            logging.error(\"WebDriver not initialized. Cannot extract data.\")\n",
    "            return None\n",
    "        try:\n",
    "            logs = self.driver.get_log('browser')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get browser logs: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "        for log_entry in reversed(logs):\n",
    "            message = log_entry.get('message', '')\n",
    "            if 'updateMessageArea:' in message:\n",
    "                match = re.search(r'updateMessageArea:\\s*(\\d+)\\s*,\\s*(\\d{1,2}:\\d{2})\\s*,\\s*([\\d,]+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)', message)\n",
    "                if match:\n",
    "                    display_date_id = match.group(1).strip()\n",
    "                    display_time = match.group(2).strip()\n",
    "                    current_value_mw_str = match.group(3).replace(',', '').strip()\n",
    "                    temperature_c_str = match.group(4).strip()\n",
    "                    try:\n",
    "                        current_value_mw = float(current_value_mw_str) if current_value_mw_str else None\n",
    "                        temperature_c = float(temperature_c_str) if temperature_c_str else None\n",
    "                        data_dict = {\n",
    "                            'scrape_timestamp_utc': datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'display_date_id': display_date_id,\n",
    "                            'display_time': display_time,\n",
    "                            'current_value_MW': current_value_mw,\n",
    "                            'temperature_C': temperature_c\n",
    "                        }\n",
    "                        logging.info(f\"Data extracted: {data_dict}\")\n",
    "                        return data_dict\n",
    "                    except ValueError as ve:\n",
    "                        logging.error(f\"Error converting extracted data: {ve}. Raw: val='{current_value_mw_str}', temp='{temperature_c_str}'\")\n",
    "        logging.warning(\"Relevant 'updateMessageArea' log not found or data parsing failed.\")\n",
    "        return None\n",
    "\n",
    "    def scrape_once(self):\n",
    "        if not self.driver:\n",
    "            logging.warning(\"WebDriver not available. Attempting to re-initialize.\")\n",
    "            self._initialize_driver()\n",
    "            if not self.driver:\n",
    "                 logging.error(\"Failed to re-initialize WebDriver. Aborting scrape_once.\")\n",
    "                 return None\n",
    "        try:\n",
    "            logging.info(f\"Navigating to URL: {self.url}\")\n",
    "            self.driver.get(self.url)\n",
    "            logging.info(\"Waiting for page load and data (10 seconds)...\")\n",
    "            time.sleep(10)\n",
    "            return self.extract_data_from_console()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during scrape_once: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            logging.info(\"Closing WebDriver.\")\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error quitting WebDriver: {e}\", exc_info=True)\n",
    "            finally:\n",
    "                self.driver = None\n",
    "\n",
    "# --- Function for a single scrape and update cycle (with robust try-except for read_parquet) ---\n",
    "def perform_scrape_and_update(scraper, lakefs_s3_path, storage_options):\n",
    "    \"\"\"\n",
    "    Performs one cycle of scraping, processing, and updating data to LakeFS.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting new scrape and update cycle ---\")\n",
    "    existing_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Attempting to read existing Parquet from: {lakefs_s3_path}\")\n",
    "        existing_df = pd.read_parquet(lakefs_s3_path, storage_options=storage_options)\n",
    "        logging.info(f\"Successfully read {len(existing_df)} rows from existing Parquet file.\")\n",
    "        if 'scrape_timestamp_utc' in existing_df.columns:\n",
    "            existing_df['scrape_timestamp_utc'] = pd.to_datetime(existing_df['scrape_timestamp_utc'], errors='coerce')\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Parquet file not found at {lakefs_s3_path} (FileNotFoundError). This is expected if it's the first run. A new file will be created.\")\n",
    "    except Exception as e:\n",
    "        error_message = str(e).lower()\n",
    "        # Check for common \"not found\" messages from S3/PyArrow\n",
    "        if \"no such file or directory\" in error_message or \\\n",
    "           \"nosuchkey\" in error_message or \\\n",
    "           \"nosuchbucket\" in error_message or \\\n",
    "           isinstance(e, pd.errors.EmptyDataError): # EmptyDataError can occur if file is empty/unreadable\n",
    "            logging.warning(f\"Parquet file not found or unreadable at {lakefs_s3_path} (Error: {type(e).__name__} - {e}). This is expected if it's the first run or an empty/corrupt file. A new/updated file will be created.\")\n",
    "        else:\n",
    "            logging.error(f\"Unexpected error reading Parquet file from {lakefs_s3_path}: {type(e).__name__} - {e}\", exc_info=True)\n",
    "            # For now, allow script to continue with an empty existing_df\n",
    "\n",
    "    new_data_dict = scraper.scrape_once()\n",
    "\n",
    "    if new_data_dict:\n",
    "        logging.info(f\"Scraped new data: {new_data_dict}\")\n",
    "        new_df = pd.DataFrame([new_data_dict])\n",
    "        new_df['scrape_timestamp_utc'] = pd.to_datetime(new_df['scrape_timestamp_utc'], errors='coerce')\n",
    "\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True) if not existing_df.empty else new_df\n",
    "\n",
    "        key_cols_for_dedup = ['display_date_id', 'display_time']\n",
    "        if all(col in combined_df.columns for col in key_cols_for_dedup) and 'scrape_timestamp_utc' in combined_df.columns:\n",
    "            logging.info(\"Deduplicating data...\")\n",
    "            combined_df.sort_values('scrape_timestamp_utc', ascending=False, inplace=True, na_position='last')\n",
    "            deduplicated_df = combined_df.drop_duplicates(subset=key_cols_for_dedup, keep='first')\n",
    "            logging.info(f\"Rows after deduplication: {len(deduplicated_df)}\")\n",
    "        else:\n",
    "            deduplicated_df = combined_df\n",
    "            logging.warning(\"Skipping deduplication due to missing key columns.\")\n",
    "\n",
    "        try:\n",
    "            if 'display_date_id' in deduplicated_df.columns and 'display_time' in deduplicated_df.columns:\n",
    "                deduplicated_df['temp_datetime_sort'] = pd.to_datetime(\n",
    "                    deduplicated_df['display_date_id'] + ' ' + deduplicated_df['display_time'],\n",
    "                    format='%Y%m%d %H:%M', errors='coerce'\n",
    "                )\n",
    "                deduplicated_df.sort_values(\n",
    "                    by=['temp_datetime_sort', 'scrape_timestamp_utc'],\n",
    "                    ascending=[True, True], inplace=True, na_position='last'\n",
    "                )\n",
    "                deduplicated_df.drop(columns=['temp_datetime_sort'], inplace=True)\n",
    "            elif 'scrape_timestamp_utc' in deduplicated_df.columns:\n",
    "                 deduplicated_df.sort_values('scrape_timestamp_utc', ascending=True, inplace=True, na_position='last')\n",
    "            logging.info(\"Sorted final DataFrame.\")\n",
    "        except Exception as sort_e:\n",
    "            logging.warning(f\"Could not perform full sort: {sort_e}. Using basic sort if possible.\")\n",
    "            if 'scrape_timestamp_utc' in deduplicated_df.columns: # Fallback sort\n",
    "                 deduplicated_df.sort_values('scrape_timestamp_utc', ascending=True, inplace=True, na_position='last')\n",
    "\n",
    "        try:\n",
    "            logging.info(f\"Saving {len(deduplicated_df)} rows to LakeFS: {lakefs_s3_path}\")\n",
    "            deduplicated_df.to_parquet(\n",
    "                lakefs_s3_path,\n",
    "                storage_options=storage_options,\n",
    "                index=False, engine='pyarrow', compression='snappy'\n",
    "            )\n",
    "            logging.info(\"Successfully saved data to LakeFS.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save DataFrame to LakeFS: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logging.warning(\"No new data was scraped in this cycle.\")\n",
    "    logging.info(\"--- Scrape and update cycle finished ---\")\n",
    "\n",
    "def run_scraper_periodically(interval_minutes=5):\n",
    "    ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY_ID\", \"access_key\")\n",
    "    SECRET_KEY = os.getenv(\"LAKEFS_SECRET_ACCESS_KEY\", \"secret_key\")\n",
    "    LAKEFS_ENDPOINT = os.getenv(\"LAKEFS_ENDPOINT_URL\", \"http://lakefs-dev:8000/\")\n",
    "    REPO_NAME = \"dataset\"\n",
    "    BRANCH_NAME = \"main\"\n",
    "    TARGET_PARQUET_FILE_PATH = \"egat_datascraping/egat_realtime_power_history.parquet\"\n",
    "    lakefs_s3_path = f\"s3a://{REPO_NAME}/{BRANCH_NAME}/{TARGET_PARQUET_FILE_PATH}\"\n",
    "\n",
    "    storage_options = {\n",
    "        \"key\": ACCESS_KEY,\n",
    "        \"secret\": SECRET_KEY,\n",
    "        \"client_kwargs\": {\n",
    "            \"endpoint_url\": LAKEFS_ENDPOINT\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Configured LakeFS Target Path: {lakefs_s3_path}\")\n",
    "    logging.info(f\"Scraping interval set to {interval_minutes} minutes.\")\n",
    "\n",
    "    scraper = None\n",
    "    try:\n",
    "        scraper = EGATRealTimeScraper()\n",
    "        if not scraper.driver:\n",
    "            logging.error(\"WebDriver could not be initialized. Terminating process.\")\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            perform_scrape_and_update(scraper, lakefs_s3_path, storage_options)\n",
    "            \n",
    "            wait_seconds = interval_minutes * 60\n",
    "            logging.info(f\"Waiting for {interval_minutes} minutes ({wait_seconds} seconds) before next cycle...\")\n",
    "            time.sleep(wait_seconds)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Process interrupted by user (KeyboardInterrupt).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred in the main loop: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if scraper:\n",
    "            scraper.close()\n",
    "        logging.info(\"Scraping process terminated.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scraper_periodically(interval_minutes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
